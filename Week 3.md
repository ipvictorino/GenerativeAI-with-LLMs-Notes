# Week 3

Owner: Inês
Status: Not started

### Learning Objectives

---

- Describe how RLHF uses human feedback to improve the performance and alignment of large language models
- Explain how data gathered from human labelers is used to train a reward model for RLHF
- Define chain-of-thought prompting and describe how it can be used to improve LLMs reasoning and planning abilities
- Discuss the challenges that LLMs face with knowledge cut-offs, and explain how information retrieval and augmentation techniques can overcome these challenges
- Responsible AI

[W3.pdf](Week%203/W3.pdf)

# Reinforcement learning with human feedback

Issues with LLM

- Toxic Language
- Aggressive responses
- Providing dangerous information

Ideally the LLMs should align with important human values HHH (Helpful, Honest, Harmless)

Additional fine-tuning with human feedback helps to better align models with human preferences, decreasing toxixity and generation of incorrect information

## Reinforcement learning from human feedback (RLHF)

- Uses Reinforcement Learning (RL)
- Popular technique to fine-tune LLM with human feedback
- Results on a model better align with human preferences, maximizing helpfulness, minimizing harm and avoiding dangerous topics
- Makes use of a reward model to assess LLM completions, which are then passed to RL algorithm (PPO) to update weights based on the reward. Iterate over cycle

### Reinforcement Learning

- Type of ML in which an agent learns to make decisions related to a specific goal by taking actions in an environment with the objective of maximizing some notion of cummulative reward
- The agent continually learns from its experiences by taking actions, observing the resulting changes on the environment, and receiving rewards or penalties, based on the outcomes of its actions. By iterating through this process, the agent gradually refines its strategy or policy to make better decisions and increase its chances of success.
    
    ![Untitled](Week%203/Untitled.png)
    
- **RL algorithm** is used to update model weights (e.g. Proximal Policy Optimization (PPO))
- **Action Space**: All possible positions the agent can choose
- **RL Policy**: The agent makes decisions by following a strategy (RL policy). The goal is to learn optimal policy for a given environment that maximizes the rewards - iterative process.
- **Playout/Rollout**: Series of actions and corresponding states

[**Proximal Policy Optimization (PPO)**](Week%203/Proximal%20Policy%20Optimization%20(PPO).md)

### RLHF

Reinforcement learning: fine-tune LLMs

- Agent RL Policy: Policy that guides the actions is the LLM
- Objective: Generate human-aligned text (helpfull, accurate, and non-toxic)
- Environment: Context window
- State: Current context that the model comsiders before taking an action
- Action: Act of generating text
- Action Space: Token Vocabulary - all possible tokens the model can choose from to generate completion

![Untitled](Week%203/Untitled%201.png)

- Reward can be evaluated:
    - Human-evaluator - compare completions against metric (e.g. toxic/non toxic)
    - Reward model - additional model to classify LLM’s outputs and evaluate the degree of alignment with human preferences
        - Uses smaller number of human examples to train secondary model via traditional supervised learning methods.
- RL algorithm: PPO or Q-learning

## Obtaining feedback from humans

- First step: Select LLM (suggestion to start with Instruction-based LLM)
- Second Step: Using prompt Dataset to generate a number of different responses (completions= for each prompt using LLM
- Third Step: Collect feedback from human labelers on the completions generated by the LLM
    1. Define what criterion you want the humans to assess the completions on
    2. For the prompt-response sets that have been generated, obtain human feedback through labelers workforce
    3. Repeat process for many prompt-completion sets, building up a dataset that can be used to train the reward model that will ultimately be used as reward model
- Step 4: Prepare labeled data for training
    1. Convert rankings into pairwise training data for the reward model, i.e. all possible pair of completions from the available choices
    2. For each pair assign a reward of 1 for preferred response and 0 to the least preferred
    3. Reorder prompts so that the preferred option comes first. The logic for this is because the reward model expects rge preffered completion $Y_j$ first
    
    ![image.png](Week%203/image.png)
    

## Reward model

Train model to predict preferred completion from $\{y_j, y_k\}$ for prompt $x$

- For a given prompt $x$, the reward model learns to favor the human-preferred completion $y_ j$, while minimizing the lock sigmoid off the reward difference, $r_j-r_k$.
    
    $$
    loss = log(\sigma(r_j-r_k))
    $$
    
- Use the reward model as binary classifier to provide the reward value (**logits**) for each prompt-completion pair
    
    
    | Tommy loves television | Logits | Probabilities |
    | --- | --- | --- |
    | Positive class (not hate) | 3.171875 | 0.996093 |
    | Negative class (hate) | -2.609275 | 0.003080 |
    
    | Tommy hates gross movies | Logits | Probabilities |
    | --- | --- | --- |
    | Positive class (not hate) | -0.535156 | 0.337890 |
    | Negative class (hate) | 0.137695 | 0.664062 |
    
    **Logits**: Unnormalized model outputs before applying any activation function
    

### Summary of Using the Reward Model in RLHF

![image.png](Week%203/image%201.png)

![image.png](Week%203/image%202.png)

![image.png](Week%203/image%203.png)

To align a language model (LLM) with human preferences, you start with a well-performing model. The process involves these steps:

1. **Initial Prompt and Completion**: Pass a prompt from your dataset to the LLM to generate a completion.
2. **Reward Model Evaluation**: Send the prompt-completion pair to the reward model, which evaluates it based on human feedback it was trained on, and returns a reward value.
    - High reward values indicate more aligned responses.
    - Low reward values indicate less aligned responses.
3. **Reinforcement Learning Update**: Use the reward value to update the LLM weights, aiming to improve alignment with human preferences.
4. **Iteration**: Repeat the process over several iterations (epochs), gradually refining the model. Higher reward scores over time indicate successful alignment.

The iterative process continues until the model meets predefined evaluation criteria, such as a threshold for helpfulness or a maximum number of steps.

The final stage we obtain the human-aligned LLM.

## RL drawbacks - Reward Hacking

- Agent learns to cheat the system by favoring actions that maximize the reward received even if the actions don’t align well with the original objective
- Examples: Preference of words or phrases that result in high scores for the metric being aligned, but that reduce the overall quality of the language
    
    > “This product is…” “…the most awesome, most incredible thing ever.”
    > 
    
    > “This product is…” “…beautiful love and world peace all around.”
    > 
    

To prevent from happening, it is possible to use the initial instruct LLM as performance reference

- Weights of reference model are frozen
- During training, each prompt is passed to both models, generating a completion by the reference LLM and the intermediate LLM updated model, which can be compared by calculating a value called Kullback-Leibler divergence, or **KL divergence** for short.
    
    ![image.png](Week%203/image%204.png)
    

### KL Divergence

| KL Divergence |
| --- |
| Statistical measure of how different two probability distributions are. |
| Used to compare the completions of the two models, and determine how much the updated model has diverged from the reference |
- Calculated for each generated token across the whole vocabulary of the LLM (10-Nk of tokens)
- KL divergence is added as a term to the reward calculation, penalizing the updated model if it shifts to far from the reference LLM

[KL divergence](Week%203/KL%20divergence%20c4a8fc2aef0e453d989974415b6a0571.md)

The full flow with KL Divergence Shift Penalty can be memory-intensive, requiring two copies of the LLM - one frozen and one updated from PPO. Therefore, it may also be beneficial to integrate PEFT techniques.

In this case, only the weights of a PEFT adapter are updated, not the full weights of the LLM. This means that the same underlying LLM can be used for both the reference model and the PPO model, which you update with a trained PEFT parameters.

![image.png](Week%203/image%205.png)

## Evaluate the human-aligned LLM

Evaluate the toxicity score

1. Create a baseline toxicity score with reward model that can assess toxic language
prompt → Instruct LLM → completion → Reward Model → Toxicity 0.14
2. Evaluate newly human aligned model on the same dataset and compare the scores
prompt → Human-aligned LLM → completion → Reward Model → Toxicity 0.09

## Scaling human feedback

Creating a trained reward model for RLHF requires significant human effort and large teams of labelers, making it a resource-intensive process.

Solution: Self-Supervision - **Constitutuional AI**

### Constitutional AI

| Constitutional AI |
| --- |
| Proposed in 2022 by Anthropic’s researchers |
| Method for training model using a set of rules and principles that govern the model’s behavior |
- Constitution = Rules + Sample prompts
- Model is trained to self critique and revise its responses to comply with these principles
- Usefull to adress some problems associated with RLHF (e.g. compromising confidential data)
- Model is trained in two distinct phases:
    1. Supervised learning:
        1. Read teaming: prompt the model in ways that try to get it to generate harmful responses
        2. Response, critique, and revision: Ask model to critique its own harmful responses according to the constitutional principles and revise them to comply to those rules
        3. Fine-tune model using pairs of read-team prompt and revised constitutional responses
        
        ![Source: Bai et al. 2022, “Constitutional AI: Harmlessness from AI Feedback”](Week%203/image%206.png)
        
        Source: Bai et al. 2022, “Constitutional AI: Harmlessness from AI Feedback”
        
    2. Reinforcement Learning:
        
        Applies Reinforcement Learning from AI Feedback (RLAIF) - Similar to RLHF, but instead uses artificial completions from the model trained in Step 1.
        
        1. Apply prompt to generate completions using model from step 1
        2. Ask the model which of the responses is preferred acording to constitutional principles
        3. Use model preference dataset to train a reward model
        4. Use reward model to further fine-tune using RL algorithm like PPO
    
    ![Source: Bai et al. 2022, “Constitutional AI: Harmlessness from AI Feedback”](Week%203/image%207.png)
    
    Source: Bai et al. 2022, “Constitutional AI: Harmlessness from AI Feedback”
    

# LLM-powered applications

The focus now will be on application integration

- Optimize the LLM to function in deployment (e.g. latency, needed compute budget, …)
- Additional resources that the LLM may need, i.e. augmenting the LLM
    - Interactions with external data / other applications
    - Define API interface where the model will be consumed

## Optimize and deploy model for inference

LLMs present inference challenges in terms of computing and storage requirements as long as presenting low latency for consuming applications → Challenges persist whether you deploy LLM on premises or to the cloud

Solutions:

- Reduce size of the LLM
    
    
    **Distillation**
    Uses larger model (”Teacher”) to train smaller model (”Student”). 
    Student is used for inference to lower storage and compute budget
    
    **Quantization**
    Transforms model weights to lower precision representation, such as 16-bit 
    
    **Pruning**
    Removes redundant model parameters that contribute little to the model’s performance
    

### Distillation

- Uses larger “trainer” model to train smaller “student” model
- Student learns to statistically mimic the teacher’s behavior. It does so in:
    - Final prediction layer
        
        OR
        
    - Final prediction layer + hidden layers
- Process:
    1. Start fine-tuned LLM as teacher and create smaller LLM for student
    2. Freeze teacher models’ weights and use it to generate completions for training data
    3. At the same time, generate completions for training data using student model varying temperature setting ($T>1$)
    4. Calculate distillation loss, which represents the knowledge distilation between the two models
    5. In parallel, student model is trained to generate predictions based on ground thruth training data, without varying temperature setting ($T=1$)
    6. Calculate student loss between hard predictions and hard labels
- Notation:
    - Teacher’s model completions: **Soft labels**
    - Student’s model completions (T varying): **Soft predictions**
    - Student’s model completions (T fixed = 1): **Hard predictions**
    - Student’s ground truth: **Hard labels**
- Application:
    - Not as effective for decoder-only models
    - Really good for encoder-only models

![image.png](Week%203/image%208.png)

**Distillation Loss**

Uses probability distribution over tokens that is produced by the teacher model's softmax layer

The temperature parameter is added to the softmax functions to make the probability distribution broader and less peaked. This helps the student model learn a more generalized and diverse set of token probabilities, improving its ability to mimic the teacher model effectively.

**Student Loss**

The combined distillation and student losses are used to update the student LLM’s weights via backpropagation

Objective $\min{L^{DIST}+L^{STUDENT}}$

### Post-Training Quantization (PTO)

- Reduces precision of model weights
- Can be applied:
    - Just to model weights
    - Both weights and activation layers
- Requires calibration step to statistically capture the dynamic range of the original parameter values

### Pruning

- Reduce model size for inference by removing weights that are not contributing much for model performance (e.g. with values close or equal to zero)
- Pruning methods:
    - Full retraining of the model
    - PEFT/LoRA
    - Post-training pruning
- In practice, the impact may not be much if only a small percentage of the model weights are close to zero

## Augment Model and Build LLM-powered applications

Challenges that cannot be solved by training alone

- Internal knowledge held by a model cuts off at the moment of pretraining.
- Struggle with complex math.
- Hallucinations

This section focuses on presenting techniques to overcome the challenges described above

An LLM (reasoning engine) application requires an **orchestrator**

- Manages the transfer of the user input to the LLM and the return of completions
- Enables technologies that augments the LLM at runtime
    - Connecting to external datasources (RAG)
    - Connecting to existing APIs of other applications
    - It can also decide what actions to take based on the information contained in the output of the LLM
- Examples: Langchain

### Retrieval Augmented Generation (RAG)

| Retrieval Augmented Generation (RAG) |
| --- |
| Framework for building LLM-powered systems that make use of external datasources |
- Overcomes the knowledge cutoff issue
- Avoids hallucinations or cofabulations by having access to more specific data
- Helps the model in understanding knowledge-specific matter
- Avoids model fine-tuning / retraining
- Useful in cases where LLM needs to access data that may not have seen before

[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

- Proposed in 2020 by Facebook researchers
- **Retriever** is at the heart of this implementation
    - Query encoder and external data source (vectorstore)
    - The encoder takes the user’s input prompt and encodes it (embeddings) for easy retrieval on the vectorstore
    - Returns the best single or group of documents from the data source
- The output of the retriever is later combined with the original user query, forming an expanded prompt, used to generate a completion.
- External Information Sources:
    - Documents
    - Wikis
    - Expert Systems
    - Web Pages
    - Databases
    - Vectorstores (particularly good for LLM applications since they contain vector representations of the texts)
- Considerations:
    - Data must fit inside context window
    - Data must be in a format hat allows its relevance to be assessed at inference time: embedding vectors

Vectorstores $\neq$ Vector databases

- Vector databases are a particular implementation of vectorstore, where each element is identified by a key (enabling citations e.g.)

### Interacting with external applications

- Allows the model to interact with the broader world
- Can be used for LLMs to trigger actions
- Can be used for LLMs to connect to other programming resources (e.g. python interpreter)

Trigger actions

- Completions must contain important information
    1. Plan Actions: LLM must generate a set of instructions so that the application knows and understands what actions to take
    2. Format Outputs: Completion needs to be formatted in a way that the external application can understand
    3. Validate actions: Collect information to validate an action

**Plan Actions**

Steps to reproduce

Step 1: Check Order ID

Step 2: Request Label

Step 3: Verify user email

Step 4: Email user label

**Format outputs**

Step 1: `select count(*) from orders where order_id = 21104`

### LLM reasoning and planning with chain-of-thought (CoT)

Complex reasoning can be challenging for LLMs → One strategy to solve this is through **chain-of-tought (CoT)**

[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

- Breaking down problem using a step-by-step approach for one or few-shot inference
- *Reasoning steps*: Intermediate steps/calculations

![image.png](Week%203/image%209.png)

![image.png](Week%203/image%2010.png)

Even though CoT provides great support for complex reasoning and math calculations, it has its limits, especially when tasks require precise calculations.

One technique to overcome this issue is to let LLM interact with applications that are good at math (Program-aieded Language Models)

### Program-aided language models (PAL)

[PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)

- Presented by Luyu Gao and collaborators at Carnegie Mellon University in 2022
- Basic idea: LLM is paired with an external code interpreter to carry out calculations
- Makes use of CoT prompting to generate executable python scripts, which are then passed to interpreter to execute
- The output format is specified to the model by including examples for one or few short inference in the prompt
- Reasoning text is passed as comment, starting with `#`
- Variables are declared based on the text in each reasoning step. Their values are assigned directly (e.g. `$var_1 = 1$`), as calculations (e.g. `$var_2=2*3$`, or using other variables (e.g. `$var_3 = var_1 + var_2$`)

![Source: Gao et al. 2022, “PAL: Program-aided Language Models”](Week%203/image%2011.png)

Source: Gao et al. 2022, “PAL: Program-aided Language Models”

Now to automate and connect the full process you need an **orchestrator**

It may be needed to manage multiple decision points, validation actions, and calls to external applications. How can you use the LLM to power a more complex application? 

### ReAct: Reasoning and action

[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

- Proposed by researchers at Princeton and Google in 2022
- Framework that **helps** LLMs plan out and execute complex workflows
- Prompting strategy that combines chain of thought reasoning with action planning
- The paper developed a series of complex prompting structured examples based on problems from Hot Pot QA - a multi-step question answering benchmark, and fever - a benchmark that uses Wikipedia passages to verify facts. The examples show how to reason through a problem and decide on actions to take that move it closer to a solution.
- These prompts have a certain structure (see below) and it repeats the cycle as many times as is necessary to obtain the final answer
    
    ![Source: Yao et al. 2022, “ReAct: Synergizing Reasoning and Acting in Language Models”](Week%203/image%2012.png)
    
    Source: Yao et al. 2022, “ReAct: Synergizing Reasoning and Acting in Language Models”
    

ReAct Structure

| ❓ Question: Problem that requires advanced reasoning and multiple steps to solve | Which magazine was started first, Arthur’s Magazine or First for Women? |
| --- | --- |
| 💭 Thought: Reasoning step that identifies how the model will tackle the problem and identify an action to take | I need to search Arthur’s Magazine and First for Women, and find which one was started first |
| 🎬 Action: An external task that the model can carry out from aln allowed set of actions | search[Arthur’s Magazine] |
| 🔎 Observation: The result of carrying out the action | “Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century” |
- For the action, in the case of the  ReAct framework, the authors created a small Python API to interact with Wikipedia through three allowed actions:
    - search[entity]: Looks for a wikipedia entry about a particular topic
    - lookup[string]: Searches for a string on a Wikipedia page
    - finish[answer]: Model uses it when it has determined the answer

Full text of instructions

https://towardsai.net/p/l/finetuning-llms-for-react

> Solve a question answering task with interleaving Thought, Action, Observation steps. 
Thought can reason about the current situation, and Action can be three types:
(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.
(3) Finish[answer], which returns the answer and finishes the task.
Here are some examples.
> 

Frameworks for developing applications powered by language models are in active development. One solution that is being widely adopted is called **LangChain.**

### LangChain

- Framework that provides modular pieces that contain the components necessary to work with LLMs
    - Prompt Templates
    - Memory that can be used to store interactions with LLM
    - Includes pre-built tools that allow to make calls to external datasets and various APIs
    - **Agents** can be used to interpret the input from the user and determine which tool or tools to use (agent for PAL, ReAct, …)
- Connecting a selection of the aforementioned components (tools, prompt templates, memory, agent) together results in a **chain.** There are sets of predefined chains that Langchain provides

When developing applications with LLMs, remember that the model's ability to reason and plan actions is influenced by its size. 

Larger models are generally more effective for advanced prompting techniques like PAL or ReAct. Smaller models may require additional fine-tuning to handle structured prompts and complex tasks, which can slow down development. 

Starting with a large, capable model and collecting user data during deployment can help train and fine-tune a smaller model for future use.

## Other considerations

Key components required for end-to-end LLM-powered applications:

- Infrastructure: Provides compute, storage and network to serve LLMs as well as to host your application components
- LLM Models: Include foundation or custom fine-tuned models
- Information Sources: Using RAG techniques
- Store and capture outputs: Feedback or completions storage
- Additional tools and frameworks used to easily implement some technologies like ReAct, PAL, …
- Application Interface: website, restapi

 

![image.png](Week%203/image%2013.png)

Amazon Sagemaker JumpStart is a model hub and covers several diagram components including the infrastructure, the LLM itself, the tools and frameworks, and even an API to invoke the model.

# Key takeaways

This week, the focus was on aligning models with human preferences such as helpfulness, harmlessness, and honesty using reinforcement learning with human feedback (RLHF). RLHF is an effective technique for fine-tuning models, leveraging existing reward models and human alignment datasets to quickly initiate the alignment process. This method helps improve model alignment, reduce toxicity, and ensure safer deployment in production.

Important optimization techniques for model inference were also covered, including distillation, quantization, and pruning. These methods help reduce the model size and minimize hardware resource requirements for serving LLMs in production.

Additionally, methods to enhance model performance in deployment were explored, such as structured prompts and connections to external data sources and applications. LLMs can serve as powerful reasoning engines in applications, and frameworks like Langchain facilitate the rapid development, deployment, and testing of LLM-powered applications.

# Reading

[Reading Resources](Week%203/Reading%20Resources.md)

# Ongoing Research - Responsible AI

With the growth of AI comes the recognition that we must all use it responsibly.

Special challenges of responsible generative AI:

- Toxicity
- Hallucinations
- Intellectual Property

**Toxicity**

LLM returns responses that can be potentially harmful or discriminatory towards protected groups or protected attributes

Mitigated by:

- Careful curation of training data
- Train guardrail models to filter out unwanted contents
- More diverse group of annotators, also providing more guidance

**Hallucinations**

Something that it is not true or that it really seems true but is not → LLM generates factually incorrect content

How to mitigate?

- Educate users about how generative AI works
- Add disclaimers
- Augment LLMs with independent, verified citation databases
- Have data tracebacks: by attributing generated output to particular pieces of training data and cross validating result
- Define intended / unintended use cases

**Intellectual Property**

Ensure people aren’t plagiarizing, make sure there aren’t any copyright issues

How to mitigate?

- Mix of technologies, policy, and legal mechanisms
- Incorporate a system of governance to make sure that every stakeholder is doing what they need to do to prevent this from happening
- Machine “unlearning”: concept that protected content or its effects on generative AI outputs are reduced or removed
- Filtering and blocking approaches

## Responsibly build and use generative AI models

- Define use cases: the more specific/narrow, the better
- Assessing risks for each use case
- Evaluate performance for each use case
- Iterate over entire AI lifecycle
- Issue governance policies throughout the lifecycle and accountability measures for every stakeholder involved.

# Active fields of research

- Water marking and fingerprinting - adding stamp or signature in a piece of data so that we can always traceback
- Models to help determine if content was AI generated or not
- Responsible AI
- Scale models and predict performance
- Neurosymbolic AI: combining structured knowledge with symbolic methods